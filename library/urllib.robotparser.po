# Copyright (C) 2001-2020, Python Software Foundation
# This file is distributed under the same license as the Python package.
# Maintained by the python-doc-es workteam.
# docs-es@python.org /
# https://mail.python.org/mailman3/lists/docs-es.python.org/
# Check https://github.com/python/python-docs-es/blob/3.8/TRANSLATORS to
# get the list of volunteers
#
msgid ""
msgstr ""
"Project-Id-Version: Python 3.8\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-12 19:43+0200\n"
"PO-Revision-Date: 2021-01-08 15:24-0600\n"
"Last-Translator: \n"
"Language: es\n"
"Language-Team: python-doc-es\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.0\n"

#: ../Doc/library/urllib.robotparser.rst:2
msgid ":mod:`urllib.robotparser` ---  Parser for robots.txt"
msgstr ":mod:`urllib.robotparser` ---  Analizador para robots.txt"

#: ../Doc/library/urllib.robotparser.rst:10
msgid "**Source code:** :source:`Lib/urllib/robotparser.py`"
msgstr "**Código fuente:** :source:`Lib/urllib/robotparser.py`"

#: ../Doc/library/urllib.robotparser.rst:20
msgid ""
"This module provides a single class, :class:`RobotFileParser`, which answers "
"questions about whether or not a particular user agent can fetch a URL on "
"the web site that published the :file:`robots.txt` file.  For more details "
"on the structure of :file:`robots.txt` files, see http://www.robotstxt.org/"
"orig.html."
msgstr ""
"Este módulo proporciona una clase única, :class:`RobotFileParser`, que "
"responde preguntas sobre si un agente de usuario en particular puede obtener "
"una URL en el sitio web que publicó el archivo :file:`robots.txt`. Para "
"obtener más detalles sobre la estructura de los archivos :file:`robots.txt`, "
"consulte http://www.robotstxt.org/orig.html."

#: ../Doc/library/urllib.robotparser.rst:28
msgid ""
"This class provides methods to read, parse and answer questions about the :"
"file:`robots.txt` file at *url*."
msgstr ""
"Esta clase proporciona métodos para leer, analizar y responder preguntas "
"acerca de :file:`robots.txt`"

#: ../Doc/library/urllib.robotparser.rst:33
msgid "Sets the URL referring to a :file:`robots.txt` file."
msgstr "Establece la URL que hace referencia a un archivo :file:`robots.txt`."

#: ../Doc/library/urllib.robotparser.rst:37
msgid "Reads the :file:`robots.txt` URL and feeds it to the parser."
msgstr "Lee la URL :file:`robots.txt` y la envía al analizador."

#: ../Doc/library/urllib.robotparser.rst:41
msgid "Parses the lines argument."
msgstr "Analiza el argumento *lines*."

#: ../Doc/library/urllib.robotparser.rst:45
msgid ""
"Returns ``True`` if the *useragent* is allowed to fetch the *url* according "
"to the rules contained in the parsed :file:`robots.txt` file."
msgstr ""
"Retorna ``True`` si el *useragent* tiene permiso para buscar la *url* de "
"acuerdo con las reglas contenidas en el archivo :file:`robots.txt` analizado."

#: ../Doc/library/urllib.robotparser.rst:51
msgid ""
"Returns the time the ``robots.txt`` file was last fetched.  This is useful "
"for long-running web spiders that need to check for new ``robots.txt`` files "
"periodically."
msgstr ""
"Retorna la hora en que se recuperó por última vez el archivo ``robots.txt``. "
"Esto es útil para arañas web de larga duración que necesitan buscar nuevos "
"archivos ``robots.txt`` periódicamente."

#: ../Doc/library/urllib.robotparser.rst:57
msgid ""
"Sets the time the ``robots.txt`` file was last fetched to the current time."
msgstr ""
"Establece la hora a la que se recuperó por última vez el archivo ``robots."
"txt`` hasta la hora actual."

#: ../Doc/library/urllib.robotparser.rst:62
msgid ""
"Returns the value of the ``Crawl-delay`` parameter from ``robots.txt`` for "
"the *useragent* in question.  If there is no such parameter or it doesn't "
"apply to the *useragent* specified or the ``robots.txt`` entry for this "
"parameter has invalid syntax, return ``None``."
msgstr ""
"Retorna el valor del parámetro ``Crawl-delay`` de ``robots.txt`` para el "
"*useragent* en cuestión. Si no existe tal parámetro o no se aplica al "
"*useragent* especificado o la entrada ``robots.txt`` para este parámetro "
"tiene una sintaxis no válida, devuelve ``None``."

#: ../Doc/library/urllib.robotparser.rst:71
msgid ""
"Returns the contents of the ``Request-rate`` parameter from ``robots.txt`` "
"as a :term:`named tuple` ``RequestRate(requests, seconds)``. If there is no "
"such parameter or it doesn't apply to the *useragent* specified or the "
"``robots.txt`` entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"Retorna el contenido del parámetro ``Request-rate`` de ``robots.txt`` como "
"una :term:`tupla nombrada` ``RequestRate(requests, seconds)``. Si no existe "
"tal parámetro o no se aplica al *useragent* especificado o la entrada "
"``robots.txt`` para este parámetro tiene una sintaxis no válida, devuelve "
"``None``."

#: ../Doc/library/urllib.robotparser.rst:81
msgid ""
"Returns the contents of the ``Sitemap`` parameter from ``robots.txt`` in the "
"form of a :func:`list`. If there is no such parameter or the ``robots.txt`` "
"entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"Retorna el contenido del parámetro ``Sitemap`` de ``robots.txt`` en forma "
"de :func:`list`. Si no existe tal parámetro o la entrada ``robots.txt`` para "
"este parámetro tiene una sintaxis no válida, devuelve ``None``."

#: ../Doc/library/urllib.robotparser.rst:89
msgid ""
"The following example demonstrates basic use of the :class:`RobotFileParser` "
"class::"
msgstr ""
"El siguiente ejemplo demuestra el uso básico de la clase :class:"
"`RobotFileParser`:"

#: ../Doc/library/urllib.robotparser.rst:12
msgid "WWW"
msgstr ""

#: ../Doc/library/urllib.robotparser.rst:12
msgid "World Wide Web"
msgstr ""

#: ../Doc/library/urllib.robotparser.rst:12
msgid "URL"
msgstr ""

#: ../Doc/library/urllib.robotparser.rst:12
msgid "robots.txt"
msgstr ""
